{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title Download Camelyon16 embeddings\n",
        "# @markdown This code is designed to download and extract a dataset called \"Camelyon16 embeddings\".\n",
        "!curl -o dataset.zip https://ailb-web.ing.unimore.it/publicfiles/miccai_dasmil_checkpoints/cam_feats.zip\n",
        "!unzip dataset.zip"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "nT-8ebdcmy3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Prepare Environment\n",
        "# @markdown This section of code is primarily focused on setting up the necessary environment for a machine learning project, likely involving PyTorch and related libraries.\n",
        "import torch\n",
        "import os\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install nystrom-attention\n",
        "!pip install wandb\n",
        "!git clone https://github.com/aimagelab/mil4wsi.git\n",
        "!cd mil4wsi"
      ],
      "metadata": {
        "cellView": "form",
        "id": "aLAzr4Lgq8qV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load Libraries\n",
        "# @markdown This code snippet is dedicated to importing necessary libraries and modules that will be used in the rest of the program.\n",
        "\n",
        "Loading Libraries and Modules\n",
        "This code snippet is dedicated to importing necessary libraries and modules that will be used in the rest of the program.\n",
        "from torch_geometric.data import Dataset\n",
        "import glob\n",
        "from torch_geometric.data import data\n",
        "from torch_geometric.loader import DataLoader\n",
        "import sys\n",
        "sys.path.append(\"/content/mil4wsi\")\n",
        "from utilsmil4wsi.test import test\n",
        "import argparse\n",
        "from models import selectModel\n",
        "import wandb\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "import torch\n",
        "import os\n",
        "import wandb\n",
        "from argparse import Namespace\n",
        "import time\n",
        "import tqdm"
      ],
      "metadata": {
        "cellView": "form",
        "id": "u_vvp7czG1RM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_root=\"camGraph_23/processed\""
      ],
      "metadata": {
        "id": "V9jrhrVRnvQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dataset\n",
        "# @markdown This code defines a custom dataset class called CustomDataset which inherits from the Dataset class from the torch_geometric.data module.\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self,root,data_type):\n",
        "        self.path=os.path.join(root,data_type,\"*\")\n",
        "        self.data=glob.glob(self.path)\n",
        "        print(self.data)\n",
        "        self.slides=[torch.load(self.data[idx]) for idx in range(len(self.data))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.slides[idx]\n",
        "        return sample\n",
        "train_dataset=CustomDataset(data_root,\"train\")\n",
        "test_dataset=CustomDataset(data_root,\"test\")\n",
        "train_loader=DataLoader(train_dataset,batch_size=1,shuffle=True)\n",
        "test_loader=DataLoader(test_dataset,batch_size=1,shuffle=False)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wzRsNcLqnnTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title General Parser\n",
        "# @markdown This function, get_args(), is designed to handle command-line arguments or parameters for the program. It uses the argparse module to define, parse, and organize these arguments into a structured format.\n",
        "# @markdown Common hyperparameters shared  between models: lr,weight_decay,seed,n_epoch,n_classes,input_size.\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser(description='hyperparameters')\n",
        "\n",
        "    # Optimization arguments\n",
        "    group1 = parser.add_argument_group(\"optimization\")\n",
        "    group1.add_argument('--lr', default=0.0002,\n",
        "                        type=float, help='learning rate')\n",
        "    group1.add_argument('--weight_decay', default=0.00001,\n",
        "                        type=float, help='Weight decay [5e-3]')\n",
        "\n",
        "    # GNN arguments\n",
        "    group2 = parser.add_argument_group(\"gnn\")\n",
        "    group2.add_argument('--residual', default=False, action=\"store_true\",)\n",
        "    group2.add_argument('--num_layers', default=1, type=int,\n",
        "                        help='number of Graph layers')\n",
        "    group2.add_argument('--dropout', default=True, action=\"store_true\")\n",
        "    group2.add_argument('--dropout_rate', default=0.2, type=float)\n",
        "    group2.add_argument('--layer_name', default=\"GAT\",\n",
        "                        type=str, help='layer graph name')\n",
        "    group2.add_argument('--heads', default=3, type=int,\n",
        "                        help='layer graph name')\n",
        "\n",
        "    # Training arguments\n",
        "    group3 = parser.add_argument_group(\"training\")\n",
        "    group3.add_argument('--seed', default=12, type=int,\n",
        "                        help='seed for reproducibility')\n",
        "    group3.add_argument('--n_epoch', default=200,\n",
        "                        type=int, help='number of epochs')\n",
        "\n",
        "    # Dimensions arguments\n",
        "    group4 = parser.add_argument_group(\"dimensions\")\n",
        "    group4.add_argument('--n_classes', default=1, type=int,\n",
        "                        help='Number of output classes [2]')\n",
        "    group4.add_argument('--c_hidden', default=256,\n",
        "                        type=int, help='intermediate size ')\n",
        "    group4.add_argument('--input_size', default=384,\n",
        "                        type=int, help='input size ')\n",
        "\n",
        "    # Dataset arguments\n",
        "    group5 = parser.add_argument_group(\"dataset\")\n",
        "    group5.add_argument('--scale', default=\"1\", type=str,\n",
        "                        help='scale resolution')\n",
        "    group5.add_argument('--dataset', default=\"cam\", type=str,\n",
        "                        choices=[\"cam\", \"lung\"], help='input size ')\n",
        "    group5.add_argument('--datasetpath',  type=str, help='dataset path')\n",
        "\n",
        "    # Distillation arguments\n",
        "    group6 = parser.add_argument_group(\"distillation\")\n",
        "    group6.add_argument('--lamb', default=1, type=float, help='lambda')\n",
        "    group6.add_argument('--beta', default=1, type=float, help='beta')\n",
        "    group6.add_argument('--temperature', default=1.5, type=float, help='temperature')\n",
        "    group6.add_argument('--add_bias', default=True,action=\"store_true\")\n",
        "    group6.add_argument('--max', default=True,action=\"store_true\")\n",
        "    group6.add_argument('--checkpoint', default=None,type=str, help='checkpoint')\n",
        "\n",
        "    parser.add_argument('--tag', default=\"split\", type=str, help='train strategy')\n",
        "    parser.add_argument('--modeltype', default=\"DSMIL\", type=str, help='train strategy')\n",
        "    parser.add_argument('--project', default=\"decider-geom\", type=str, help='project name for wandb')\n",
        "    parser.add_argument('--model', default=\"decider-geom\", type=str, help='project name for wandb')\n",
        "    parser.add_argument('--wandbname', default=\"main\", type=str, help='project name for wandb')\n",
        "\n",
        "\n",
        "    group7= parser.add_argument_group(\"submitit\")\n",
        "    group7.add_argument('--partition', default=\"prod\",type=str,help='partition name')\n",
        "    group7.add_argument('--time', default=120, type=float, help='job duration')\n",
        "    group7.add_argument('--nodes', default=1, type=int, help='number of jobs')\n",
        "    group7.add_argument('--job_name', default=\"dasmil\",type=str,help=\"job name\")\n",
        "    group7.add_argument('--mem', default=32, type=int, help='ram requested GB')\n",
        "    group7.add_argument('--job_parallel', default=10, type=int, help='number of jobs in parallel')\n",
        "    group7.add_argument('--logfolder', default=\"logfolder\", type=str, help='log folder location name')\n",
        "\n",
        "\n",
        "    #buffermil parameters\n",
        "    group8= parser.add_argument_group(\"submitit\")\n",
        "\n",
        "    group8.add_argument(\"--randomstore\", default=False,help=\"ramdom sampling during the buffer storage\")\n",
        "    group8.add_argument(\"--bufferaggregate\", default=\"mean\",choices=[\"mean\",\"max\",\"diffmax\"], help=\"type of buffer aggregation\")\n",
        "    group8.add_argument(\"--ntop\", default=10, help=\"number of patches stored in the buffer per each image\")\n",
        "    group8.add_argument('--buffer_freq',default=10, type=int, help='frequency to update the buffer')\n",
        "    filtered_args = [arg for arg in sys.argv if not arg.startswith('-f')]\n",
        "    args = parser.parse_args(filtered_args[2:])\n",
        "    return args"
      ],
      "metadata": {
        "cellView": "form",
        "id": "LaLCA3s3y41l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args= get_args()\n",
        "print(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-SVOmf4xcIQ",
        "outputId": "1893d558-2f09-4c28-c235-424443c46735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(lr=0.0002, weight_decay=0.005, residual=False, num_layers=1, dropout=True, dropout_rate=0.2, layer_name='GAT', heads=3, seed=12, n_epoch=200, n_classes=1, c_hidden=256, input_size=384, scale='1', dataset='cam', datasetpath=None, lamb=1, beta=1, temperature=1.5, add_bias=True, max=True, checkpoint=None, tag='split', modeltype='DSMIL', project='decider-geom', model='decider-geom', wandbname='main', partition='prod', time=120, nodes=1, job_name='dasmil', mem=32, job_parallel=10, logfolder='logfolder', randomstore=False, bufferaggregate='mean', ntop=10, buffer_freq=10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Select Model\n",
        "# @markdown This code snippet creates an interactive dropdown menu within the Jupyter Notebook to allow the user to select a machine learning model.\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Crea un menu a discesa\n",
        "dropdown = widgets.Dropdown(\n",
        "\n",
        "    options=['DSMIL', 'ABMIL', 'MaxPooling',\"MeanPooling\",\"TransMIL\",\"Buffermil\"],\n",
        "    value='DSMIL',\n",
        "    description='Choose:',\n",
        ")\n",
        "\n",
        "# Mostra il widget\n",
        "display(dropdown)\n",
        "\n",
        "# Recupera il valore selezionato\n",
        "def on_value_change(change):\n",
        "    print(f\"Selected: {change['new']}\")\n",
        "\n",
        "dropdown.observe(on_value_change, names='value')\n",
        "args.modeltype=dropdown.value"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_Vpuvd0xIHWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=selectModel(args)\n",
        "model.kl=None"
      ],
      "metadata": {
        "id": "deS1EoTF0G6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Training Method\n",
        "# @markdown This code defines a function called train that is responsible for training the machine learning model.\n",
        "def train(model,train_loader,test_loader,args):\n",
        "   # Initialize wandb run\n",
        "    run = wandb.init(project=\"MIL\",name=args.modeltype)\n",
        "    epochs = args.n_epoch\n",
        "    wd=args.weight_decay\n",
        "    lr=args.lr\n",
        "    model.train()\n",
        "    model = model.cuda()\n",
        "    loss_module_instance = BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(\n",
        "        0.5, 0.9), weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, 0.000005)\n",
        "    # Test the initial model\n",
        "    with torch.no_grad():\n",
        "        start_test = time.time()\n",
        "        metrics = test(model, testloader=test_loader)\n",
        "        end_test = time.time()\n",
        "        avg_score_higher_test, avg_score_lower_test, auc_value_higher_test, auc_value_lower_test, predictions, _, labels = metrics\n",
        "\n",
        "        wandb.log({\n",
        "            \"acc_higher_test\": avg_score_higher_test,\n",
        "            \"acc_lower_test\": avg_score_lower_test,\n",
        "            \"auc_higher_test\": auc_value_higher_test,\n",
        "            \"epoch\": -1,\n",
        "            \"lr\": scheduler.get_last_lr()[0]\n",
        "        })\n",
        "    BestPerformance = 0\n",
        "    # Start training\n",
        "    for idx,epoch in tqdm.tqdm(enumerate(range(epochs)),desc=\"epochs\"):\n",
        "        start_training = time.time()\n",
        "        if hasattr(model,\"preloop\"):\n",
        "            model.preloop(epoch,train_loader)\n",
        "        # Iterate over the training data\n",
        "        for _, data in enumerate(train_loader):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            data = data.cuda()\n",
        "            x, edge_index, childof, level = data.x, data.edge_index, data.childof, data.level\n",
        "            # Check if additional edge indices are present\n",
        "            if data.__contains__(\"edge_index_2\") and data.__contains__(\"edge_index_3\"):\n",
        "                edge_index2, edge_index3 = data.edge_index_2, data.edge_index_3\n",
        "            else:\n",
        "                edge_index2 = None\n",
        "                edge_index3 = None\n",
        "\n",
        "            try:\n",
        "                results = model(x, edge_index, level, childof,edge_index2, edge_index3)\n",
        "            except:\n",
        "                continue\n",
        "            bag_label = data.y.float()\n",
        "            loss = model.compute_loss(loss_module_instance, results, bag_label)\n",
        "            wandb.log({\"loss\": loss})\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        end_training = time.time()\n",
        "        scheduler.step()\n",
        "        start_test = time.time()\n",
        "        metrics = test(model, testloader=test_loader)\n",
        "        end_test = time.time()\n",
        "        avg_score_higher_test, avg_score_lower_test, auc_value_higher_test, auc_value_lower_test, predictions, _, labels = metrics\n",
        "\n",
        "        wandb.log({\n",
        "            \"acc_higher_test\": avg_score_higher_test,\n",
        "            \"acc_lower_test\": avg_score_lower_test,\n",
        "            \"auc_higher_test\": auc_value_higher_test,\n",
        "            \"epoch\": epoch,\n",
        "            \"lr\": scheduler.get_last_lr()[0]\n",
        "        })\n",
        "\n",
        "    wandb.finish()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6gyKkA8kvQac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Start Training\n",
        "train(model,train_loader,test_loader,args)"
      ],
      "metadata": {
        "id": "oTV2Lel34bci"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}